{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4e9e3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys \n",
    "from langchain_core.prompts import PromptTemplate,ChatPromptTemplate\n",
    "from langchain_google_genai  import GoogleGenerativeAI,GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "539a3786",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv,find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "groq_api_key=os.getenv('GROQ_API_KEY')\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8470421e",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=ChatGroq(groq_api_key=groq_api_key,\n",
    "             model_name=\"llama-3.3-70b-versatile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da096acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"C:\\\\KaiBot\\\\Human_Segmentation_Research.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a643d2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc3b034b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "final_documents = text_splitter.split_documents(doc[:20])  # splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3820524",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cffd44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96a8e5a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_20904\\2107674846.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
      "c:\\MultimodalRAg\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.from_documents(final_documents, embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ba37ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorstore = FAISS.from_documents(final_documents,embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1cb051b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=ChatPromptTemplate.from_template(\n",
    "\"\"\"\n",
    "You are an AI research assistant with expertise in analyzing academic papers. \n",
    "Answer the user's question based only on the provided context.\n",
    "If the answer is not found in the context, state that clearly.\n",
    "Context: {context}\n",
    "Questions:{input}\n",
    "\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d51f67d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fceb2d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriver=vectorstore.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1444a21b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response time : 0.171875\n",
      "Based on the provided context, it appears that the paper discusses a comparison of different methods for semantic segmentation, a task in computer vision where the goal is to assign a label to each pixel in an image. \n",
      "\n",
      "The paper computes the means of various metrics, including validation loss, training loss, validation IoU (Intersection over Union), and training IoU, for different encoders and optimizers. These means are compiled into comparative tables, allowing for clear benchmarking between methods. Additionally, graphical representations, such as bar plots, are created to visualize these trends.\n",
      "\n",
      "The paper also references various other research papers and techniques, including U-Net, SegNet, and EfficientNet, which are all deep learning architectures used for semantic segmentation. The context also mentions statistical analysis and performance evaluation of deep learning-based segmentation methods.\n",
      "\n",
      "However, the context does not provide a detailed summary of the paper's findings, conclusions, or contributions. It only provides a glimpse into the methodology and references used in the paper. Therefore, a detailed summary of the paper cannot be provided based on the given context.\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "document_chain=create_stuff_documents_chain(llm,prompt)\n",
    "retriever=vectorstore.as_retriever()\n",
    "retrieval_chain=create_retrieval_chain(retriever,document_chain)\n",
    "start=time.process_time()\n",
    "response=retrieval_chain.invoke({'input':\"Explain the summary of the paper in detail.\",\n",
    "                                 'context':\" \".join([doc.page_content for doc in final_documents[:3]])})\n",
    "print(\"Response time :\",time.process_time()-start)\n",
    "#print(response['answer'])\n",
    "if isinstance(response['answer'], list):  \n",
    "    print(response['answer'][0])  # take the first response\n",
    "else:\n",
    "    print(response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "41f80450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The methodology involves computing the means of various performance metrics, compiling them into comparative tables, and creating graphical representations (such as bar plots) to visualize trends. Specifically, the study reports six statistical measures (mean, standard deviation, minimum, maximum, skewness, and kurtosis) for both loss and Intersection over Union (IoU) on training and validation sets for different optimization algorithms (e.g., Adam, SGD, RAdam) and encoder models (e.g., U-Net, EfficientNet, ResNet)."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# The .stream() method returns a generator that yields dictionary chunks.\n",
    "# We iterate through the generator to get the response as it's created.\n",
    "for chunk in retrieval_chain.stream({'input': \"Summarize the methodology.\"}):\n",
    "    # The actual generated text is usually in the 'answer' key of the chunk.\n",
    "    if \"answer\" in chunk:\n",
    "        # Print the chunk of the answer without a newline, and flush the output\n",
    "        # to ensure it appears immediately in the console.\n",
    "        print(chunk['answer'], end=\"\", flush=True)\n",
    "\n",
    "#   print(\"\\n\\nResponse time :\", time.process_time() - start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
